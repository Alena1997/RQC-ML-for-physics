{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', data_home='./')\n",
    "X = mnist['data']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "fig, axs = plt.subplots(n, n, figsize = (n,n))\n",
    "\n",
    "for wi in range(n):\n",
    "    for wj in range(n):\n",
    "\n",
    "        axs[wi, wj].axis('off')\n",
    "        axs[wi, wj].imshow(X[wi*n+wj].reshape(28,28), cmap='gray')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization\n",
    "X = X.astype('float')\n",
    "Xm = np.mean(X, axis=0)\n",
    "Xs = np.std(X, axis=0)\n",
    "X=(X - Xm) / (Xs + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels conversion to one-hot encoding and train-test splitting\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = mnist['target']\n",
    "\n",
    "X_n = X[:]\n",
    "Y_n = Y[:]\n",
    "\n",
    "print('original', Y_n)\n",
    "t = OneHotEncoder(sparse=False, categories='auto')\n",
    "Y_work = t.fit_transform(Y_n.reshape(-1, 1))\n",
    "print('one hot', Y_work)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_work, test_size=0.2, stratify=Y_n)\n",
    "\n",
    "print('X train shape :', X_train.shape, ', Labels train shape :', Y_train.shape,\n",
    "     '\\nX test shape :', X_test.shape, ', Labels test shape :', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FF - NN in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data convertion to torch.tensor\n",
    "X_train = torch.from_numpy(X_train).to(dtype=torch.float32).to(device)\n",
    "X_test = torch.from_numpy(X_test).to(dtype=torch.float32).to(device)\n",
    "Y_train = torch.from_numpy(Y_train).to(dtype=torch.float32).to(device)\n",
    "Y_test = torch.from_numpy(Y_test).to(dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784, 800), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 10)     \n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def compute_loss(y_true, y_hat):\n",
    "    return -torch.sum(y_true*torch.log_softmax(y_hat, dim=-1))\n",
    "\n",
    "#accuracy calculation\n",
    "def calculate_accuracy(pred_one_hot, label_one_hot):\n",
    "    \n",
    "    prediction = torch.max(pred_one_hot,dim=1)[1]\n",
    "    labels = torch.max(label_one_hot,dim=1)[1]\n",
    "    \n",
    "    acc = ((prediction - labels) == 0).sum()/label_one_hot.sum()\n",
    "    \n",
    "    return acc.cpu().numpy()\n",
    "\n",
    "#minibatches\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.5e-3)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "L_train, L_test, acc_train, acc_test = [], [], [], []\n",
    "L_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    # train\n",
    "    net.train(True)\n",
    "    L = 0.\n",
    "    acc = 0.\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, Y_train, batch_size):\n",
    "        y_h = net.forward(X_batch)\n",
    "        loss = compute_loss(y_batch, y_h)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        L += loss.detach().cpu().numpy()\n",
    "    L_train.append(L/Y_train.shape[0])\\\n",
    "    \n",
    "    # test\n",
    "    net.train(False)    \n",
    "    y_h = net.forward(X_test)\n",
    "    with torch.no_grad():\n",
    "        L = compute_loss(Y_test, y_h).cpu().numpy()\n",
    "    L_test.append(L/Y_test.shape[0])\n",
    "    acc_test.append(calculate_accuracy(y_h, Y_test))\n",
    "    if epoch % 1 == 0:\n",
    "            print(\"{} epoch loss. Train : {}, Test : {}\".format(\n",
    "                                                                epoch, \n",
    "                                                                np.round(L_train[-1],2),\n",
    "                                                                np.round(L_test[-1],2)\n",
    "                                                                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (10,5))\n",
    "\n",
    "ax1.plot(L_train, label='train')\n",
    "ax1.plot(L_test, label='test')\n",
    "ax1.grid()\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(acc_train, label='train')\n",
    "ax2.plot(acc_test, label='test')\n",
    "ax2.grid()\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
